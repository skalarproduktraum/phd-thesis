# track2track — Eye Tracking for Object Tracking in Volumetric Data

In this chapter, we are going to detail the track2track strategy for augmenting biological tracking tasks with gaze data in order to make easier and faster to do.

We will first discuss the tracking problems usually encountered in biology and then detail the design process that went into track2track.

## Tracking Problems in Biology and Challenges

In cell and developmental biology, the image data generated by fluorescence microscopy is most often only a means to an end: Many tasks require e.g. exact information about positions of cells during development, or even their entire history, the so-called cell lineage tree. 

As, in contrast to e.g CT scan images, images from fluorescence microscopy don't have any well-defined color scale, and intensity might even vary across a single cell. It can therefore be very hard to follow a cell — which moves, divides, or dies — around in a developing tissue or organism. Often, the task of tracking cells is done manually over a series of time points, which can be a very tedious process. In the past, tracking software was often developed for a specific model organism, e.g. for _C. elegans_, and relied on their stereotypical development to succeed in tracking their cells. Such approaches however either fail entirely, or produce unreliable results for larger organism. For that reason, (semi)automated approaches have been developed which are independent of the model organism and can track large amounts of cells:

* _TGMM_, Tracking by Gaussian Mixture Models [@amat2014; @ckendorf:2015ch], is an offline tracking solution that works by generating oversegmented supervoxels from the original image data, then fit all cell nuclei with a Gaussian Mixture Model and evolve that through time, and finally use the temporal context of the various lineages to create the final lineage.
* _TrackMate_ [@tinevez2017] is a plugin for Fiji [@schindelin2012fiji] that provides automatic, semi-automatic, or manual tracking of single particles in imaging datasets. TrackMate is highly customizeable and allows the user to even extend it with self-developed spot detection or tracking algorithms.
* _MaMuT_, the Massive MultiView Tracker [@wolff2018], is in some sense the successor to TrackMate, allowing the user to track cells in large datasets, often originating from lightsheet microscopes, and containing multiple different views. MaMuT's viewer is based on BigDataViewer [@Pietzsch:2015hl].

All of these automated approaches however have in common that they need manual curation as a final step. All of them also make assumptions about cell shapes and model them e.g. as Gaussian blobs. The main problem we are going to address in the following is the manual curation step.

The challenges of this step are:

* Image data from fluorescence microscopy can be very inhomogeneous — as can the cells. Deriving a general algorithm that can capture even very deformed cells is a highly challenging task.
* Manually curation of cell lineages is very tedious at the moment, as the users have to go through each timepoint and connect cells between the timepoints. This is often done on a per-slice basis and by mouse, leading to a time-consuming process.

## Design Space

We intend to tackle the problems stated in the section before by using a combination of eye tracking and virtual reality. 

The user can be tasked to follow a cell with his eyes, the gaze direction recorded, and the targeted cell then determined, turning the 3-dimensional localisation problem into a 1-dimensional one — from the whole volume of image data, to a single ray through it. The human visual system, as described in the chapter [Introduction to Visual Processing], is excellent in following moving objects smoothly, and in datasets used for cell tracking, the cells can also be assumed to move smoothly. Interestingly, the particular kind of eye movement we are exploiting here, _smooth pursuits_ — see the section [Eye movements] in [Introduction to Visual Processing] for details — is rather underexplored in human-computer interaction. To the author's knowledge, only [@piumsomboon2017] use it in their _Radial Pursuit_ technique. In _Radial Pursuit_, the user can select an object by following it in a scene with his eyes, and it will become more "lensed-out" the longer he does that.

With the addition of virtual reality to the mix, we can first help the user with orientation in the dataset, and second, utilise the tracking data from the head-mounted display, consisting of head position and head orientation, for constraining the eye tracking data to remove outliers or spurious pupil detections, or even foveate the rendering of the volumetric dataset.

## Design Process

### Initial Prototype

For the initial prototype, within _scenery_ we created a virtual reality-based crowded environment consisting of many differently-sized and differently-colored boxes. A black sphere is performing random motions through them, and the user was instructed to follow this sphere, and not lose sight of them. A screenshot of the prototype can be see in \ref{fig:attentive_tracking_prototype}.

![\label{fig:attentive_tracking_prototype}2D Screenshot of the attentive tracking prototype. The sphere to be tracked can be seen in the upper left corner of the image. See the text for details.](attentive_tracking_prototype.png)

This prototype was tested with an HTC Vive on a set of 5 biologists and people from related areas that have to perform manual tracking, without telling them first that they should actually perform a tracking problem. And while manual tracking is usually perceived as a tedious, boring and annoying task, the test subjects have described following the sphere in VR as interesting and fun.

Encouraged by the positive reactions to the first, admittedly very primitive, prototype, a next, more serious prototype was planned. Just tracking head orientation and position would not be enough for the precision required, so an eye-tracking solution was integrated into the HTC Vive to procure more detailed information on where the user is looking at any point in time.

### Selecting the eye tracking hardware

For the this project, we have chosen the _Pupil_ eye trackers produced by _Pupil Labs_[@Kassner:2014kh][^pupilnote], as they provide a solution that provides both open-source software and very competitively hardware that is in addition very easy to integrate into HMDs. The software offered is available as LGPL-licensed open-source software on Github ([https://github.com/pupil-labs](https://github.com/pupil-labs)) and can be easily extended with custom plugins. 

In addition to being open, data gathered by the software is available to external applications via a ZeroMQ-based protocol — in contrast to closed-source proprietary libraries required by other products — which even enables the use of the eye tracking data over the network.

At the time of writing, HTC's extended version of their _Vive Pro_ HMD, the _Vive Pro Eye_, with integrated eye tracking hardware, is also becoming available. It will be interesting to compare the two solutions in the future, especially as the _Vive Pro Eye_ will be more competitively priced (around EUR900) than a regular _Vive_ combined with the _Pupil_ eye trackers (EUR600 for the HMD, plus EUR1100 for the eye trackers).

[^pupilnote]: The _Pupil_ HMD-based eye tracker from Pupil Labs, see [https://www.pupil-labs.com](https://www.pupil-labs.com).

## Pupil detection and calibration

### Pupil 2D and 3D detection

\begin{marginfigure}
    \includegraphics{pupil-pupil_detection.png}
    \caption{Pupil detection in the \emph{Pupil} software. Image reproduced from \citep{Kassner:2014kh}.\label{fig:Pupil2DDetection}}.
\end{marginfigure}

Using three user-defined parameters, pupil intensity range, and pupil min/max diameter, _Pupil_ extracts the pupil from the camera images as follows (from [@Kassner:2014kh]):

1. A Canny edge detector is applied to the camera image (Figure \ref{fig:Pupil2DDetection}-1),
2. Darker regions of the image are selected, based on the pupil intensity range parameter, the offset of the first histogram maximum, edges outside this area are discarded (Figure \ref{fig:Pupil2DDetection}-2),
3. the remaining edges are filtered to exclude specular reflections, e.g. from the IR LEDs, then the remaining edges are extracted into contours using connected components (Figure \ref{fig:Pupil2DDetection}-3, spectral reflections in yellow),
4. the remaining contours are filtered and split into sub-contours based on the continuity of their curvature (Figure \ref{fig:Pupil2DDetection}-4), 
5. candidate pupil ellipses are formed using least squares fitting, with the major axis within the bounds of the pupil min/max parameter, and a combinatorial search is done on the remaining contours to see which might be added to the ellipse for additional support. Resulting ellipses are evaluated based on the ratio of supporting edge length and ellipse circumference, called _confidence_ in _Pupil_, and finally
6. if the best result's confidence is above a defined threshold, the candidate ellipse is reported as result, otherwise the algorithm returns that no pupil has been found.

If 3D detection is selected in _Pupil_, the result ellipse is passed on to the algorithm described in [@Swirski:2013b12]. As we are only going to use 2D detection for _Attentive Tracking_, we are not going to detail this algorithm, but refer the interested reader to the paper instead.

We have found that the usage of the Canny edge detector used in the first step of the algorithm leads to contour overdetection, and therefore useless pupil detections, with higher-resolution camera images, and therefore used a resolution of 640x480 pixels, which provides the best tradeoff between speed, processing cost, and accuracy. We are currently exploring alternatives to the algorithm described, based on genetic algorithms.

### Calibration procedure

Eye positions, size, etc. are subject to large individual differences. It is therefore required to calibrate the eye trackers before each use, to be able to get reliable gaze data out. 

In case of regular, glasses-mounted eye trackers, _Pupil_ offers an integrated calibration procedure, while for HMD-based settings, we need to create our own calibration routine. The calibration has to be redone over time, this is not actually a negative aspect, but can help with a good integration of the calibration procedure into the virtual world presented to the user.

Our custom calibration routine works as follows:

 1. we show the user a single highlighted point located on a circle in the center of the screen, the user is instructed to follow the highlighted point with his eyes,
 2. after acquiring enough samples for calibration, the screen space position of the calibration point is sent to Pupil,
 3. the next, randomly selected point out of a set of 10 equidistant points on the circle is shown to the user,
 4. after all points have been gazed at by the user, Pupil's calibration routine will try to construct a correspondence between the gaze vectors of both eyes and the screen-space coordinates submitted,
 5. if the calibration routine is successful, the calibration interface will be hidden, and the regular application can continue. If the calibration routine is not successful, we restart from 1. until successful or until the user cancels.

\TODO{Add calibration image}

## Tracking Procedure

## Analysis of the Hedgehog

After all rays have been collected for a tracking step, all further track data is derived from the set of rays, which we call the _hedgehog_. An individual ray we dub _spine_, as it is more than just the ray with orientation and direction, but also contains information about:

* the confidence of the gaze data point it was derived from,
* the entry and exit points through the volume in volume-local coordinates (meaning for each coordinate axis $\in [0.0, 1.0]$),
* the head orientation at recording time,
* the head position at recording time,
* the timepoint of the volume it belongs to, and
* a list of samples taken in uniform spacing along the ray, along with the spacing. 

How this collection looks visually is depicted in Figure \ref{fig:hedgehog}.

![The hedgehog of a tracking step of a single cell through 100 timepoints in a Platynereis dataset. Each spine is color-coded by timepoint, with early timepoints shown in green, and later ones in yellow. Dataset courtesy of Mette Handberg-Thorsager, Tomancak Lab, MPI-CBG.\label{fig:hedgehog}](hedgehog.png)

\begin{marginfigure}
    \includegraphics{hedgehog-raw.png}
    \caption{The raw plot of the hedgehog rays. On the X axis, volume intensity along a single ray is shown, on the Y axis, time runs from top to bottom.\label{fig:rawHedgehog}}
\end{marginfigure}

\begin{marginfigure}
    \includegraphics{hedgehog-with-maxima.png}
    \caption{The same hedgehog with local maxima marked. On the X axis, volume intensity along a single ray is shown, on the Y axis, time runs from top to bottom. Local maxima are shown in red.\label{fig:labelledHedgehog}}
\end{marginfigure}

## Results

## Discussion and Future Work


